{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector - EN - Features extraction for SVM & Dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [this notebook](https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/final_classifier.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "from klepto.archives import dir_archive\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import fasttext\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "import syllables as sylla\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'svm'\n",
    "dim = 10 if MODEL == 'svm' else 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davidson et al. data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes pre:\n",
    "    0 - hate speech\n",
    "    1 - offensive language\n",
    "    2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hsd/DavidsonEtAl/perfect_data.pkl'):\n",
    "    tweets, labels = [], []\n",
    "    with open('hsd/DavidsonEtAl/labeled_data.csv', 'r') as f:\n",
    "        for d in tqdm(list(csv.reader(f))[1:]):\n",
    "            tweets.append(d[6])  # tweet\n",
    "            labels.append(d[5])  # class\n",
    "    with open('hsd/DavidsonEtAl/perfect_data.pkl', 'w') as f:\n",
    "        def chcl(c):\n",
    "            return 0 if c=='2' else 1\n",
    "        labels = list(map(chcl, labels))\n",
    "        pickle.dump((tweets, labels), f)\n",
    "else:\n",
    "    with open('hsd/DavidsonEtAl/perfect_data.pkl', 'r') as f:\n",
    "        tweets, labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes post:\n",
    "    0 - no hate\n",
    "    1 - hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets: 24783\n",
      "Labels: 24783\n"
     ]
    }
   ],
   "source": [
    "print('Tweets: {}'.format(len(tweets)))\n",
    "print('Labels: {}'.format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\",\n",
       "  0),\n",
       " ('!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!',\n",
       "  1),\n",
       " ('!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit',\n",
       "  1),\n",
       " ('!!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny', 1),\n",
       " ('!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;',\n",
       "  1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tweets[:5], labels[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "sentiment_analyzer = VS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    #hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = re.sub(hashtag_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "def get_pos_string(tweet):\n",
    "    text = preprocess(tweet)\n",
    "    tokens = basic_tokenize(preprocess(text))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = ' '.join(tag_list)\n",
    "    \n",
    "    return tag_str\n",
    "\n",
    "def pad_words(words, length):\n",
    "    if len(words) >= length:\n",
    "        return words[:length]\n",
    "    else:\n",
    "        additional = length - len(words)\n",
    "        return words + ['EMPTY']*additional\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = sylla.estimate(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59, 1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)), 2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0 if \"rt\" in words else 1\n",
    "    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised fastText wordtokens training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('hsd/DavidsonEtAl/fasttext_{}.ft'.format(MODEL)):\n",
    "    with open('hsd/DavidsonEtAl/fasttext_{}.ft'.format(MODEL), 'a') as f:\n",
    "        for t, l in list(zip(tweets, labels)):\n",
    "            f.write('__label__{} {}\\n'.format(l, preprocess(t)))\n",
    "\n",
    "# load fasttext model or train & save if none\n",
    "if os.path.exists('hsd/DavidsonEtAl/fasttext_{}.bin'.format(MODEL)):\n",
    "    ft_model = fasttext.load_model('hsd/DavidsonEtAl/fasttext_{}.bin'.format(MODEL))\n",
    "else:\n",
    "    ft_model = fasttext.train_supervised('hsd/DavidsonEtAl/fasttext_{}.ft'.format(MODEL),\n",
    "                                         lr=0.5, epoch=50, wordNgrams=3, dim=dim)\n",
    "    ft_model.save_model('hsd/DavidsonEtAl/fasttext_{}.bin'.format(MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordtoken features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordtoken_fts(data):\n",
    "    \n",
    "    sentences_words = []\n",
    "    for d in tqdm(data):\n",
    "        sentence = preprocess(d)\n",
    "        sentences_words.append(sentence.split(' '))\n",
    "    \n",
    "    opt_length = int(np.median([len(sw) for sw in sentences_words]))\n",
    "    sentences_words = [pad_words(sw, opt_length) for sw in sentences_words]\n",
    "    \n",
    "    ft_vectors = []\n",
    "    for sw in tqdm(sentences_words):\n",
    "        ft_vector = []\n",
    "        for w in sw:\n",
    "            ft_vector.extend(ft_model[w])\n",
    "        ft_vectors.append(ft_vector)\n",
    "    \n",
    "    return ft_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ce1dd696db42b1a89f5cea941bc972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24783), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c950002da834fc0908d6139e17bc2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24783), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wordtoken_features = get_wordtoken_fts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007928177,\n",
       " 0.11992569,\n",
       " 0.0028752065,\n",
       " 0.030718988,\n",
       " 0.062001467,\n",
       " -0.007022215,\n",
       " -0.07705958,\n",
       " 0.08626743,\n",
       " 0.018074136,\n",
       " 0.08626623,\n",
       " 0.16760364,\n",
       " -0.25004625,\n",
       " 0.05675336,\n",
       " 0.021296112,\n",
       " 0.1535667,\n",
       " -0.07181231,\n",
       " 0.19030358,\n",
       " 0.04006586,\n",
       " -0.32047436,\n",
       " -0.23894314,\n",
       " 0.090784736,\n",
       " -0.25651675,\n",
       " -0.09038265,\n",
       " -0.0018909777,\n",
       " -0.024543583,\n",
       " -0.014274004,\n",
       " 0.1895897,\n",
       " -0.029224055,\n",
       " -0.1547373,\n",
       " -0.23869601,\n",
       " 0.13214399,\n",
       " -0.03543946,\n",
       " -0.08781389,\n",
       " 0.009753116,\n",
       " -0.031385317,\n",
       " -0.03915471,\n",
       " 0.08630752,\n",
       " -0.06722911,\n",
       " -0.09088202,\n",
       " -0.05752908,\n",
       " -0.39256153,\n",
       " 0.5459367,\n",
       " 0.12314718,\n",
       " -0.108320236,\n",
       " -0.09030823,\n",
       " 0.008959142,\n",
       " -0.11464913,\n",
       " 0.015319895,\n",
       " 0.09448529,\n",
       " 0.034212455,\n",
       " -0.07821279,\n",
       " 0.31698993,\n",
       " 0.082358114,\n",
       " -0.07300558,\n",
       " -0.016081097,\n",
       " 0.028319005,\n",
       " -0.07943164,\n",
       " -0.027838785,\n",
       " 0.018876733,\n",
       " 0.024743862,\n",
       " -0.7822932,\n",
       " 1.3342228,\n",
       " 0.1795538,\n",
       " -0.40651023,\n",
       " -0.21574245,\n",
       " -0.039143153,\n",
       " -0.37216437,\n",
       " -0.15515819,\n",
       " 0.5185793,\n",
       " 0.4386448,\n",
       " 0.11361385,\n",
       " -0.2186819,\n",
       " 0.0061534946,\n",
       " 0.099468246,\n",
       " 0.0942772,\n",
       " -0.021535335,\n",
       " 0.08122698,\n",
       " 0.11064325,\n",
       " -0.168605,\n",
       " -0.0039097317,\n",
       " -0.022536248,\n",
       " 0.016323391,\n",
       " 0.033363186,\n",
       " 0.06503592,\n",
       " 0.08681318,\n",
       " -0.013513965,\n",
       " 0.038184524,\n",
       " 0.0037679633,\n",
       " 0.039422635,\n",
       " 0.033095818,\n",
       " -0.19713113,\n",
       " 0.17100672,\n",
       " -0.085439056,\n",
       " -0.011437364,\n",
       " -0.00085150887,\n",
       " 0.030011337,\n",
       " -0.061392352,\n",
       " -0.039526135,\n",
       " 0.08517008,\n",
       " 0.07437204,\n",
       " 0.38132882,\n",
       " -0.6363911,\n",
       " 0.02461435,\n",
       " 0.1590169,\n",
       " 0.012850397,\n",
       " 0.03627967,\n",
       " 0.13319184,\n",
       " 0.1449103,\n",
       " -0.13088624,\n",
       " -0.22179335,\n",
       " -0.1931148,\n",
       " 0.59115696,\n",
       " 0.03067511,\n",
       " -0.0976687,\n",
       " -0.13445292,\n",
       " 0.0419406,\n",
       " -0.039076388,\n",
       " -0.091336615,\n",
       " 0.08392286,\n",
       " 0.12266536,\n",
       " -0.37554896,\n",
       " 0.7749955,\n",
       " -0.00034183613,\n",
       " -0.18285505,\n",
       " -0.08906238,\n",
       " 0.09983137,\n",
       " -0.25967187,\n",
       " -0.101192616,\n",
       " 0.30588993,\n",
       " 0.33105525]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtoken_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised fastText pos training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('hsd/DavidsonEtAl/fasttext_pos_{}.ft'.format(MODEL)):\n",
    "    with open('hsd/DavidsonEtAl/fasttext_pos_{}.ft'.format(MODEL), 'a') as f:\n",
    "        for t, l in list(zip(tweets, labels)):\n",
    "            f.write('__label__{} {}\\n'.format(l, get_pos_string(t)))\n",
    "\n",
    "# load fasttext pos model or train & save if none\n",
    "if os.path.exists('hsd/DavidsonEtAl/fasttext_pos_{}.bin'.format(MODEL)):\n",
    "    ft_pos_model = fasttext.load_model('hsd/DavidsonEtAl/fasttext_pos_{}.bin'.format(MODEL))\n",
    "else:\n",
    "    ft_pos_model = fasttext.train_supervised('hsd/DavidsonEtAl/fasttext_pos_{}.ft'.format(MODEL),\n",
    "                                             lr=0.5, epoch=50, wordNgrams=3, dim=dim)\n",
    "    ft_pos_model.save_model('hsd/DavidsonEtAl/fasttext_pos_{}.bin'.format(MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech (PoS) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_fts(data):\n",
    "\n",
    "    #Get POS tags for tweets and save as a string\n",
    "    pos_sentences = []\n",
    "    for d in tqdm(data):\n",
    "        pos_string = get_pos_string(d)\n",
    "        pos_sentences.append(pos_string)\n",
    "        \n",
    "        \n",
    "    pos_tags = []\n",
    "    for ps in pos_sentences:\n",
    "        pos_tags.append(ps.split(' '))\n",
    "    \n",
    "    opt_length = int(np.median([len(pt) for pt in pos_tags]))\n",
    "    pos_tags = [pad_words(pt, opt_length) for pt in pos_tags]\n",
    "    \n",
    "    ft_vectors = []\n",
    "    for pt in tqdm(pos_tags):\n",
    "        ft_vector = []\n",
    "        for t in pt:\n",
    "            ft_vector.extend(ft_pos_model[t])\n",
    "        ft_vectors.append(ft_vector)\n",
    "    \n",
    "    return ft_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb95752f042e4d8e9a1c4565217e28cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24783), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae7bd8bdefc40b681185fc5500e82d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24783), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_features = get_pos_fts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.3302995,\n",
       " 1.2387867,\n",
       " 0.03511857,\n",
       " -0.003499597,\n",
       " 0.023812015,\n",
       " 0.036711995,\n",
       " 0.5470143,\n",
       " -0.9115662,\n",
       " -0.036720674,\n",
       " 0.23484771,\n",
       " -0.5209875,\n",
       " 1.913564,\n",
       " -0.21366146,\n",
       " 0.32891658,\n",
       " 0.110604905,\n",
       " -0.24601156,\n",
       " 0.6151405,\n",
       " 0.18846886,\n",
       " 0.5396054,\n",
       " 0.62362736,\n",
       " -0.23190664,\n",
       " 0.9580747,\n",
       " -0.22189823,\n",
       " 0.3510105,\n",
       " 0.34789371,\n",
       " 0.43311724,\n",
       " 0.14764233,\n",
       " -0.4856387,\n",
       " 0.45577163,\n",
       " 0.1420188,\n",
       " -0.17901443,\n",
       " 1.4668449,\n",
       " -0.19256817,\n",
       " -0.019572409,\n",
       " -0.11394046,\n",
       " -0.14568205,\n",
       " 0.15846448,\n",
       " -0.23181453,\n",
       " 0.072660044,\n",
       " 0.2347832,\n",
       " -0.5209875,\n",
       " 1.913564,\n",
       " -0.21366146,\n",
       " 0.32891658,\n",
       " 0.110604905,\n",
       " -0.24601156,\n",
       " 0.6151405,\n",
       " 0.18846886,\n",
       " 0.5396054,\n",
       " 0.62362736,\n",
       " -1.1092862,\n",
       " 1.9167979,\n",
       " -0.24071167,\n",
       " -0.12813602,\n",
       " -0.43814883,\n",
       " -0.20309842,\n",
       " 0.64285403,\n",
       " 0.13513944,\n",
       " 0.22155532,\n",
       " 1.4106488,\n",
       " 0.33328894,\n",
       " -0.037182942,\n",
       " 0.74806005,\n",
       " 0.16574498,\n",
       " -0.3084814,\n",
       " -0.015700206,\n",
       " 0.1743191,\n",
       " 0.02270176,\n",
       " 0.1213183,\n",
       " 0.46741793,\n",
       " -0.10187656,\n",
       " 0.64105576,\n",
       " -0.59103274,\n",
       " 0.20252323,\n",
       " -0.08733121,\n",
       " -0.1538258,\n",
       " 0.55377656,\n",
       " -0.03077027,\n",
       " -0.16004266,\n",
       " 0.68976593,\n",
       " -0.5209875,\n",
       " 1.913564,\n",
       " -0.21366146,\n",
       " 0.32891658,\n",
       " 0.110604905,\n",
       " -0.24601156,\n",
       " 0.6151405,\n",
       " 0.18846886,\n",
       " 0.5396054,\n",
       " 0.62362736,\n",
       " -0.23190664,\n",
       " 0.9580747,\n",
       " -0.22189823,\n",
       " 0.3510105,\n",
       " 0.34789371,\n",
       " 0.43311724,\n",
       " 0.14764233,\n",
       " -0.4856387,\n",
       " 0.45577163,\n",
       " 0.1420188,\n",
       " -0.5717629,\n",
       " 0.28829575,\n",
       " -0.26668993,\n",
       " -0.059767853,\n",
       " -0.7390851,\n",
       " -0.26292717,\n",
       " 0.81541044,\n",
       " -0.25539106,\n",
       " 0.6995668,\n",
       " 0.45215088,\n",
       " 0.16460843,\n",
       " 0.94822687,\n",
       " 0.5854309,\n",
       " 0.06236801,\n",
       " 0.28817594,\n",
       " 0.039062936,\n",
       " -0.11610164,\n",
       " -0.04952631,\n",
       " 0.04915422,\n",
       " 0.26784313]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ed2016c58e4f2a917ead43ec8dc3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24783), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "other_features = np.array([other_features(t) for t in tqdm(tweets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.4000e+00,  8.6710e+01,  2.8000e+01,  1.1200e+00,  1.2700e+02,\n",
       "         1.4000e+02,  2.5000e+01,  2.5000e+01,  2.3000e+01,  0.0000e+00,\n",
       "         1.2000e-01,  8.8000e-01,  4.5630e-01,  0.0000e+00,  1.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00],\n",
       "       [ 1.7000e+00,  1.1128e+02,  1.5000e+01,  9.3750e-01,  7.7000e+01,\n",
       "         8.5000e+01,  1.6000e+01,  1.6000e+01,  1.6000e+01,  2.3700e-01,\n",
       "         0.0000e+00,  7.6300e-01, -6.8760e-01,  0.0000e+00,  1.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00],\n",
       "       [ 3.4000e+00,  1.0617e+02,  1.9000e+01,  9.5000e-01,  9.3000e+01,\n",
       "         1.2000e+02,  2.1000e+01,  2.0000e+01,  1.8000e+01,  5.3800e-01,\n",
       "         0.0000e+00,  4.6200e-01, -9.5500e-01,  0.0000e+00,  2.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00],\n",
       "       [-2.1000e+00,  1.2469e+02,  7.0000e+00,  8.7500e-01,  3.8000e+01,\n",
       "         6.2000e+01,  9.0000e+00,  8.0000e+00,  8.0000e+00,  0.0000e+00,\n",
       "         3.4400e-01,  6.5600e-01,  5.6730e-01,  0.0000e+00,  2.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00],\n",
       "       [ 5.4000e+00,  1.0235e+02,  2.4000e+01,  9.2310e-01,  1.2200e+02,\n",
       "         1.3700e+02,  2.6000e+01,  2.6000e+01,  2.3000e+01,  2.4900e-01,\n",
       "         8.1000e-02,  6.6900e-01, -7.7620e-01,  1.0000e+00,  1.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features and feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "features = np.concatenate([wordtoken_features, pos_features, other_features],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 267)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = dir_archive('hsd/DavidsonEtAl/X_y_{}'.format(MODEL), {'features': features, 'labels': labels}, serialized=True)\n",
    "archive.dump()\n",
    "del archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
